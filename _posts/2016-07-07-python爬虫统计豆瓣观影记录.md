---
layout: post
title: python爬虫统计豆瓣观影记录
categories: ['coding']
tags: ['python', '爬虫']
published: True
img: 50

---

说来大四这一年真的看了好多电影啊。。。因为平时有用豆瓣电影记录自己已看电影的习惯，而且博客上也维护了一个影单，所以趁着这几天有时间就想着干脆写一个统计自己大四这一年观影记录的小程序吧！

打算最终分三种方式统计信息：
    
- 统计不同类型的电影数目并绘制柱状图

- 统计不同语言的电影数目并绘制饼状图
    
- 统计每个月的观影数目并绘制趋势图

因此需要获取的信息电影信息有：电影名、观影时间、电影类型、电影语言；另外后期可能还想统计其他信息，干脆把评分、自己的评论、电影导演和制片国家/地区都爬取下来。

说干就干，首先设计一下程序的逻辑架构，具体分为以下几步：

    1. 获取博客已看影单上的所有电影名，并存入数据库的movie表中；
    
    2. 利用爬虫爬取豆瓣电影上自己的观影记录，包括（电影名、评分、观影时间、自己的评论、电影类型、电影导演、制片国家/地区 以及 电影语言），获取这些信息后写入数据库的douban_movies表；
    
    3. 利用douban_movies表与movie表left join，并更新movie表出电影名以外的其他信息；
    
    4. 通过相关模块包把数据库中的数据写入excel表中并生成图表；
    
    5. 将生成的图表转换为图片并保存；
    
这个思路是通的，而且看起来操作起来难度也不大。在这里要解释一下为什么我不直接使用douban_movies表来统计，而要基于博客的观影记录来统计，原因在于我是去年十一月份才真正意义上开始使用豆瓣电影标记功能的，因此一些很久以前（like:小时候^_^）看过的电影也在豆瓣上标记下来了，所以这部分数据是要剔除的。

当然对于很早之前就开始使用豆瓣电影的朋友来说，只需对程序稍加改造即可生成自己的观影记录了，当然后文也会提供改造后的代码链接。这里就展示下我的个人统计数据吧（个人豆瓣id:[yinwoods](https://www.douban.com/people/yinwoods/)）：

![观影时间统计](http://7xlnl2.com1.z0.glb.clouddn.com/post50-Chart1.jpg)

这里2015年11月份观影指数暴涨是因为我标记了很多很早以前看过的电影。。。

![观影语言统计](http://7xlnl2.com1.z0.glb.clouddn.com/post50-Chart2.jpg)

![观影类型统计](http://7xlnl2.com1.z0.glb.clouddn.com/post50-Chart3.jpg)

下面就按照逻辑架构开始编写程序吧！

#### 1. 获取博客上已看影单的所有电影名，并存入数据库中的movie表
    
虽然简单，但是还是要说下思路，以影单页面的url作为入口，爬虫访问后利用beautifulsoup解析文档树，提取所有的电影名并将电影名插入数据表中，为了防止每次运行程序不会插入重复数据，插入之前需要判重操作。代码如下：

{% highlight python linenos %}

#获取url 对应 HTML 源码
def get_html(self, url):
    request = urllib.request.Request(url)

    try: #代理：110.81.238.173:8088
        proxy_support = urllib.request.ProxyHandler({'http': '110.81.238.173:8088'})
        opener = urllib.request.build_opener(proxy_support)
        urllib.request.install_opener(opener)

        page = urllib.request.urlopen(request)

        if page.headers.get('Content-Encoding') == 'gzip':
            return zlib.decompress(page.read(), 16+zlib.MAX_WBITS).decode('gbk', 'ignore')
        else:
            return page.read().decode(page.headers.get_content_charset(), 'ignore')
    #异常处理
    except urllib.request.HTTPError as e:
        print('HTTPERROR: ', str(e))
        return urllib.request.HTTPError
    except http.client.HTTPException as e:
        print('http.client.HTTPException: ', str(e))
        return http.client.HTTPException

#获取电影名并写入数据库movie中
def get_movies(self, url_lists):

    for url in url_lists:
        html_doc = BeautifulSoup(self.get_html(url), 'html.parser')
        div_lists = html_doc.select('#content > div > div > a > div')
        for div in div_lists:

            movie_name = div.getText()

            #查重
            query = 'SELECT COUNT(*) FROM movie WHERE movie_name = %s'
            self.cursor.execute(query, (movie_name,))
            self.conn.commit()
            res = self.cursor.fetchall()
            if not res[0][0] == 0:
                continue

            print(movie_name)

            #插入
            query = 'INSERT INTO movie(movie_name) VALUES (%s)'
            self.cursor.execute(query, (movie_name,))
            self.conn.commit()
{% endhighlight %}
    
#### 2. 利用爬虫爬取豆瓣电影上的观影信息

依然是同样的套路，首先找到这些信息的入口，找到url:https://movie.douban.com/people/用户名/collect?sort=time&amp;start=0&amp;filter=all&amp;mode=list&amp;tags_sort=count，发现链接中不包含页码，所以考虑每次获取下一页的url加入待爬队列直到队列为空（其实就是广度优先搜索~~~），代码如下：
    
```python
#从豆瓣上获取我的观影信息
def get_douban_infos(self, username):

    url_que = queue.Queue()
    url_que.put('https://movie.douban.com/people/' + username + '/collect?sort=time&amp;start=0&amp;filter=all&amp;mode=list&amp;tags_sort=count')
    while not url_que.empty():
        url = url_que.get()
        #爬取第一页
        html_doc = BeautifulSoup(self.get_html(url), 'html.parser')

        #获取下一页链接
        next_url = html_doc.select('#content > div.grid-16-8.clearfix > div.article > div.paginator > span.next > a')
        
        #如果有下一页则加入待爬队列
        if len(next_url) > 0:
            next_url = next_url[0]['href']
            url_que.put(next_url)

        html_doc = BeautifulSoup(self.get_html(url), 'html.parser')
        li_lists = html_doc.select('#content > div')[1].select('div.article > ul > li')

        for li in li_lists:

            a_label = li.select('div.item-show > div > a')[0]

            movie_name = a_label.getText().strip()
            movie_name = movie_name.split('/')[0].strip()

            #去重
            query = 'SELECT COUNT(*) FROM douban_movie WHERE movie_name = (%s)'
            self.cursor.execute(query, (movie_name,))
            self.conn.commit()
            res = self.cursor.fetchall()
            if not res[0][0] == 0:
                continue
            
            #电影类型等信息到详情页中获取更容易
            movie_detail_url = a_label['href']
            print(movie_detail_url)
            try:
                movie_info = self.get_movie_infos(movie_detail_url)
            except TypeError as e:
                print(str(e))
                print(movie_name)
                continue
            
            #利用正则表达式提取评分，注意有些电影未打评分的情况
            rate = li.select('div.item-show > div')[1].select('span')
            if len(rate) == 0:
                rate = ''
            else:
                rate = str(rate[0]['class'])
                regx = re.compile('(\d)')
                rate = regx.findall(rate)[0]

            watch_time = li.select('div.item-show > div')[1].getText().strip()

            comment = li.select('div.comment')
            #如果有评论
            if len(comment) > 0:
                comment = comment[0].getText().strip()
                if not comment.find('(1 有用)') == -1:
                    comment = comment.split(' ')[0].strip()
            else:
                comment = ''

            print(movie_info)

            movie_types = movie_info['类型']
            movie_language = movie_info['语言']
            movie_country = movie_info['制片国家/地区']
            movie_director = movie_info['导演']

            print(comment)
            query = 'INSERT INTO douban_movie(movie_name, rate, watch_time, comment, movie_types, movie_language, movie_country, movie_director) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)'
            self.cursor.execute(query, (movie_name, rate, watch_time, comment, movie_types, movie_language, movie_country, movie_director))
            self.conn.commit()
    return
```
    
在从详情页获取信息的过程中，可以采用一些技巧方便我们提取出自己想要的信息（例如代码中把信息以dict的形式返回，方便解析），代码如下：

```python
#从电影详情页获取电影类型，结果返回一个dict
def get_movie_infos(self, url):

    html_doc = BeautifulSoup(self.get_html(url), 'html.parser')
    div_info = html_doc.select('#info')[0].getText().strip().split('\n')
    res_info = {}
    for div in div_info:
        div = div.split(': ')
        if div[0] == '导演':
            res_info.update({'导演' : ','.join(div[1:])})
        elif div[0] == '制片国家/地区':
            res_info.update({'制片国家/地区' : ','.join(div[1:])})
        elif div[0] == '语言':
            res_info.update({'语言' : ','.join(div[1:])})
        elif div[0] == '类型':
            res_info.update({'类型' : ','.join(div[1:])})
    return res_info
```
    
#### 3. 利用douban_movies表与movie表left join，并更新movie表出电影名以外的其他信息；
    
这一步操作就比较简单了，其实就是对数据表的操作，但是因为自己博客上电影命名不规范的原因，因此还要做一个匹配表来处理明明不规范的case；这里就不贴函数了。
    
#### 4. 通过相关模块包把数据库中的数据写入excel表中并生成图表；
    
现在我们已经获得了自己的观影数据并存入数据库中，后面要做的就是把数据统计信息写入excel表了，这里推荐[xlsxwriter模块](http://xlsxwriter.readthedocs.io/#)（PS：其实最开始就是想通过这个程序学习下xlsxwriter模块23333），[xlsxwriter模块](http://xlsxwriter.readthedocs.io/#)的主要功能是实现了python对excel表的操作。但是xlsxwriter的一个问题是不能读取excel的数据，往往需要结合pandas模块才能发挥最大作用。当然这里我们只需要写入功能，例如统计不同类型电影数目并制作柱状图的代码如下：
    
```python
#将电影类型分类展示在柱状图中并统计数量
def show_types_in_column(self, workbook):

    worksheet = workbook.add_worksheet()

    chart = workbook.add_chart({'type' : 'column'})

    query = 'SELECT movie_name, movie_types FROM movie'
    self.cursor.execute(query)
    self.conn.commit()
    movies_lists = self.cursor.fetchall()

    movie_types_dict = {}

    for movie in movies_lists:
        movie_name, movie_types = movie
        types = movie_types.split('/')
        for type in types:
            type = type.strip()
            if type in movie_types_dict:
                movie_types_dict[type] += 1
            else:
                movie_types_dict.update({type : 1})

    # Add table data
    table_data = [['类型', '片数'],]
    for key, val in movie_types_dict.items():
        table_data.append([key, val])

    type = '类型'
    num = '数目'
    worksheet.write_column('A1', type)
    worksheet.write_column('B1', num)
    worksheet.write_column('A2', movie_types_dict.keys())
    worksheet.write_column('B2', movie_types_dict.values())

    chart.add_series({'name' : '影片数目',
                      'categories' : ['Sheet1', 1, 0, 1+len(movie_types_dict.keys()), 0],
                      'values' : ['Sheet1', 1, 1, 1+len(movie_types_dict.values()), 1],
                      'color' : 'red',
                      'data_labels' : {'value' : True}})

    #设置X轴属性
    chart.set_x_axis({
        'name' : '影片类型',
        'name_font' : {'size' : 14, 'bold' : False},
        'num_font' : {'size' : 12},
        'line' : {'none' : True},
        'major_gridlines' : {
            'visible' : True,
            'line' : {'width' : 1.5, 'dash_type' : 'dash'}
        },
        'text_axis' : True
    })

    #设置长宽
    chart.set_size({
        'x_scale' : 2.5,
        'y_scale' : 2
    })

    #设置标题
    chart.set_title({
        'name' : '15/07/01 - 16/06/30 观影类型统计'
    })

    #设置属性块
    chart.set_legend({
        'position' : 'left'
    })

    #图下方显示表格
    chart.set_table({
        'show_keys' : True,
        'font' : {'size' : 12}
    })

    worksheet.insert_chart('C1', chart)
```

#### 5.将生成的图表转换为图片并保存；

这里把excel表中的chart转为图片通过VBA编程可以比较容易地实现，但是我们的目标是通过python来实现，经过搜索了解到可以使用windows的扩展包pywin32模块来实现这个功能，具体的代码源于网上的开源代码，其实难度倒是没有，主要难点在于api的使用。
    

### 总结

最后完整的代码在这里：[观影统计代码](https://github.com/yinwoods/show_movies_info)
    
另外，也开发了删减版，只需输入豆瓣用户名即可生成相应的观影统计信息（对这个功能感兴趣的朋友应该更多吧）。

说说存在的问题，因为是调用了微软的excel中的VBA所以一定要在windows下跑并且确保安装了pywin32模块（安装比较麻烦），如果有朋友对代码功能改进兴趣的话可以email我讨论（yinwoods@163.com）
    
按照赋比兴的老套路，我也得放点感想在这里才显得逼格高啊hhhhh。
    
想一想豆瓣作为一个典型的文艺青年聚集地曾经社区那么活跃，如今已经衰落到“门前冷落鞍马稀”了，我想在互联网行业日新月异的今天，豆瓣几年来几乎一成不变，呈现出一种拒绝变化的姿态很难说不是衰落的一方面原因，拥有那么多数据为什么不多多服务人民群众，拥抱一下大数据呢？
    
最近在YY买台云主机，把博客搭建到服务器上，以后造出来的一些轮子也可以服务大众了，比如把这个做成一个小轮子，方便电影爱好者对自己的观影数据有一个直观的认知。

