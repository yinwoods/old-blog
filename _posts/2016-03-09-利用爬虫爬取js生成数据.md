---
layout: post
title: 利用爬虫爬取js生成数据
categories: ['coding']
tags: ['爬虫','python']
published: True
img: 44

---

最近实习任务就是各种爬东西，先后爬取了[车王二手车](http://www.carking001.com/ershouche/)、[车300网站](http://www.che300.com/)

基本都是用python的`urllib.request` + `BeautifulSoup`完成，其中因为车300的数据规格不统一，采取了先把所有数据爬下来保存为json格式再整理的方法。而且车300的汽车属性值太多，爬取过慢，采用了多线程的方式爬取（其实是同时运行多个程序。。。囧）。车王的爬取就要简单多了，不再一一赘述。

昨天BOSS给我布置的任务是爬取[工信部中国汽车燃料消耗量网站](http://chinaafc.miit.gov.cn/n2257/n2263/index.html?searchId=yhcx)，本来以为把之前的代码随便改改就可以用了，没想到这个网站的数据是js生成的，也就是说我直接获取html页面源码的话是得不到想要的数据的。

参考了下网上的资料，发现这个比较有用：[Python 爬虫如何获取 JS 生成的 URL 和网页内容？](https://www.zhihu.com/question/21471960#answer-27853222)

但我刚读的时候没读懂，因为说的不够详细，我是在自己摸索出来后才发现我所做的就是答案所描述的内容。

下面以图示的方法详细说一下怎么爬取想要的内容：

> 1、在[工信部中国汽车燃料消耗量网站](http://chinaafc.miit.gov.cn/n2257/n2263/index.html?searchId=yhcx)界面按下F12，选择NetWork选项，并按下F5按键。


![](http://7xlnl2.com1.z0.glb.clouddn.com/post44-step1.png)


> 2、刷新后观察NetWork变化，找到相应发送请求的文件，在这里是一个名为`searchIndex.jsp`的文件，鼠标左键单击查看请求详细信息，可以观察到这个GET请求发送了一个param参数，从而我们可以猜想正是由于浏览器向服务器发送了param参数我们得以获得该页的数据。

![](http://7xlnl2.com1.z0.glb.clouddn.com/post44-step2.png)

> 3、要想证实我们的猜想，我们先来看一下访问Request URL，访问的结果如下：

![](http://7xlnl2.com1.z0.glb.clouddn.com/post44-step3.png)

> 可以发现恰好是我们当前页面数据的json格式，那之后的工作很容易想到就是获取每一页数据的json格式，我们再对数据进行整理即可。

> 4、现在我们来尝试对param进行解析，在这里使用[UrlEncode编码/UrlDecode解码 - 站长工具](http://tool.chinaz.com/Tools/URLEncode.aspx)进行解析，解析后的结果如下：

>> param值为`%7B%22goPage%22%3A1%2C%22orderBy%22%3A%5B%7B%22orderBy%22%3A%22pl%22%2C%22reverse%22%3Afalse%7D%5D%2C%22pageSize%22%3A10%2C%22queryParam%22%3A%5B%7B%22type%22%3A%22number%22%2C%22shortName%22%3A%22sqgk%22%2C%22min%22%3A0%2C%22max%22%3A1000000%7D%2C%7B%22type%22%3A%22number%22%2C%22shortName%22%3A%22sjgk%22%2C%22min%22%3A0%2C%22max%22%3A1000000%7D%2C%7B%22type%22%3A%22number%22%2C%22shortName%22%3A%22zhgk%22%2C%22min%22%3A0%2C%22max%22%3A1000000%7D%5D%7D`

>> 解析后为`{"goPage":1,"orderBy":[{"orderBy":"pl","reverse":false}],"pageSize":10,"queryParam":[{"type":"number","shortName":"sqgk","min":0,"max":1000000},{"type":"number","shortName":"sjgk","min":0,"max":1000000},{"type":"number","shortName":"zhgk","min":0,"max":1000000}]}`

> 可以注意到一个goPage的属性，可以猜想其后面的值即为对应的页码。再比较第二页的param值可以很容易定位到页码的位置是`%7B%22goPage%22%3A` + page + `...`

到这里我们就完成了基本的分析工作，后面要做的就是coding啦。

###最后附上我的代码：

{% highlight python %}
import urllib
import urllib.request
import json
import mysql.connector

#工信部燃油爬虫类
class chinaafcCrawler:

    #建立数据库连接
    def __init__(self):
        self.id = 0
        self.conn = mysql.connector.connect(user='root', password='root', host='localhost', database='chinaafc')
        self.cursor = self.conn.cursor(buffered=True)
    
    #释放数据库连接
    def __del__(self):
        self.cursor.close()
        self.conn.close()

    #获取url对应的HTML源码
    def getHtml(self, url):
        headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/48.0.2564.116 Chrome/48.0.2564.116 Safari/537.36',
            'Host': 'chaxun.miit.gov.cn',
            'Request Method': 'GET',
        }
        headers = dict(headers)
        request = urllib.request.Request(url, headers=headers)

        #使用代理，避免被封IP
        try:
            proxy_support = urllib.request.ProxyHandler({'http' : '120.198.231.22:82'})
            opener = urllib.request.build_opener(proxy_support)
            urllib.request.install_opener(opener)

            page = urllib.request.urlopen(request)
            return page.read().decode('utf8')

        except urllib.request.HTTPError as e:
            print("HTTPERROR: " + str(e))

        return

    #在获取的json中取出所有页面数目和所有条目数目，这里借用了json操作
    def getPageAndContentNum(self, url):
        str = self.getHtml(url).rstrip()
        str = "{" + str[str.find('totalContentNum')-1:-2]
        dataJson = json.loads(str)
        return (dataJson['totalContentNum'], dataJson['totalPageNum'])

    #截取我们想要的内容保存为json并进行解析，插入数据库
    def getHtmlDocSoup(self, html):
        total = html.rstrip()
        left = total.find('[')
        right = total.rfind(']')
        item = total[left:right+1]

        total = total[left:right+1]

        while True:
            left = total.find('{')
            if left == -1:
                break
            right = total.find('}')
            item = total[left:right+1]

            total = total[right+1:]

            dataJson = json.loads(item)

            keyLists = ['scqy', 'clzl', 'clxh', 'tymc', 'fdjxh', 'rllx', 'pl', 'tgrq',
                        'edgl', 'bsqlx', 'qdxs', 'zczbzl', 'zdsjzzl', 'sqgk', 'zhgk',
                        'sjgk', 'baID', 'sygjbz', 'sjlrsj', 'bz']

            for key in keyLists:
                if key not in dataJson.keys():
                    dataJson[key] = ''

            self.id += 1;
            query = "SELECT COUNT(*) FROM ryxhl WHERE id = %s"
            self.cursor.execute(query, (self.id,))
            res = self.cursor.fetchall()

            #去重
            if res[0][0] == 1:
                continue


            query = "INSERT INTO ryxhl(id, scqy, clzl, clxh, tymc, fdjxh, rllx, pl, tgrq, edgl, " \
                    "bsqlx, qdxs, zczbzl, zdsjzzl, sqgk, zhgk, sjgk, baID, sygjbz, sjlrsj, bz" \
                    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)"

            print(self.id)
            data = (self.id, dataJson['scqy'], dataJson['clzl'], dataJson['clxh'], dataJson['tymc'], dataJson['fdjxh'],
                    dataJson['rllx'], dataJson['pl'], dataJson['tgrq'], dataJson['edgl'], dataJson['bsqlx'],
                    dataJson['qdxs'], dataJson['zczbzl'], dataJson['zdsjzzl'], dataJson['sqgk'], dataJson['zhgk'],
                    dataJson['sjgk'], dataJson['baID'], dataJson['sygjbz'], dataJson['sjlrsj'], dataJson['bz'], )

            self.cursor.execute(query, data)
            self.conn.commit()

            '''
            print("\n\n\n生产企业：" + dataJson['scqy'] +
                  "\n车辆种类：" + dataJson['clzl'] +
                  "\n车辆型号：" + dataJson['clxh'] +
                  "\n通用名称：" + dataJson['tymc'] +
                  "\n发动机型号：" + dataJson['fdjxh'] +
                  "\n燃料类型：" + dataJson['rllx'] +
                  "\n排量：" + dataJson['pl'] +
                  "\n通告日期：" + dataJson['tgrq'] +
                  "\n额定功率：" + dataJson['edgl'] +
                  "\n变速器类型：" + dataJson['bsqlx'] +
                  "\n驱动型式：" + dataJson['qdxs'] +
                  "\n整车整备质量：" + dataJson['zczbzl'] +
                  "\n最大设计总质量：" + dataJson['zdsjzzl'] +
                  "\n市区工况：" + dataJson['sqgk'] +
                  "\n综合工况：" + dataJson['zhgk'] +
                  "\n市郊工况：" + dataJson['sjgk'] +
                  "\n备案号：" + dataJson['baID'] +
                  "\nXX国际标准：" + dataJson['sygjbz'] +
                  "\nXXXX时间：" + dataJson['sjlrsj'] +
                  "\n备注：" + dataJson['bz']
                  )
            '''

def main():

    crawler = chinaafcCrawler()

    url = "http://chaxun.miit.gov.cn/asopCmsSearch/searchIndex.jsp?params=%257B%2522goPage%2522%253A1%252C%2522orderBy%2522%253A%255B%257B%2522orderBy%2522%253A%2522pl%2522%252C%2522reverse%2522%253Afalse%257D%255D%252C%2522pageSize%2522%253A10%252C%2522queryParam%2522%253A%255B%257B%2522shortName%2522%253A%2522allRecord%2522%252C%2522value%2522%253A%25221%2522%257D%255D%257D&callback=jsonp1457489227664&_=1457489227689"
    (totalContentNum, totalPageNum) = crawler.getPageAndContentNum(url)
    print("共有", totalPageNum, "页,", totalContentNum, "条信息")
    for page in range(1, totalPageNum+1):
        url = "http://chaxun.miit.gov.cn/asopCmsSearch/searchIndex.jsp?params=%257B%2522goPage%2522%253A" + str(page) + "%252C%2522orderBy%2522%253A%255B%257B%2522orderBy%2522%253A%2522pl%2522%252C%2522reverse%2522%253Afalse%257D%255D%252C%2522pageSize%2522%253A10%252C%2522queryParam%2522%253A%255B%257B%2522shortName%2522%253A%2522allRecord%2522%252C%2522value%2522%253A%25221%2522%257D%255D%257D&callback=jsonp1457489227664&_=1457489227689"
        crawler.getHtmlDocSoup(crawler.getHtml(url))

if __name__  == "__main__":
    main()
{% endhighlight %}
